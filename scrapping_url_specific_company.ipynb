{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1640f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries as needed\n",
    "from selenium import webdriver\n",
    "from selenium import webdriver   # for webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # for implicit and explict waits\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm.notebook import tqdm # to create loadbard in for loop\n",
    "import os\n",
    "from datetime import date\n",
    "import time\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13033aed",
   "metadata": {},
   "source": [
    "For now dowload all earning call transcript for a specified ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a1352",
   "metadata": {},
   "source": [
    "### url to use for apple\n",
    "https://seekingalpha.com/api/v3/symbols/aapl/transcripts?filter[until]=undefined&id=aapl&include=author%2CprimaryTickers%2CsecondaryTickers%2Csentiments&isMounting=true&page[size]=20\n",
    "\n",
    "https://seekingalpha.com/api/v3/symbols/aapl/transcripts?filter[until]=1501632962&id=aapl&include=author%2CprimaryTickers%2CsecondaryTickers%2Csentiments&isMounting=false&page[size]=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29a3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_for_scrap(ticker, publish_time_min ):\n",
    "    # accept publis_time_min = None\n",
    "    return_per_request  = 20 #the one used by seekingalpha by default\n",
    "    ticker = ticker.lower()\n",
    "    publish_time_min = \"undefined\" if publish_time_min ==None else str(publish_time_min)\n",
    "    to_return = (\"https://seekingalpha.com/api/v3/symbols/\"+ticker+\"/transcripts?filter[until]=\"+publish_time_min\n",
    "                 +\"&id=\"+ticker\n",
    "                 +\"&include=author%2CprimaryTickers%2CsecondaryTickers%2Csentiments&isMounting=true&page[size]=\"\n",
    "                 + str(return_per_request))\n",
    "    return to_return\n",
    "def is_earning_call(element_response):\n",
    "    title = element_response[\"attributes\"][\"title\"].lower()\n",
    "    has_earning_in_title = \"earning\" in title or \"earnings\" in title\n",
    "    has_call_in_title = \"call\" in title\n",
    "    return element_response[\"type\"]==\"transcript\" and has_call_in_title and has_earning_in_title\n",
    "def get_url_id_date_earning_call(element_response):\n",
    "    date = element_response[\"attributes\"][\"publishOn\"][:10]\n",
    "    call_id = element_response[\"id\"]\n",
    "    url = element_response[\"links\"][\"self\"]\n",
    "    return url, call_id, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7d0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MMM' 'AOS' 'ABT' 'ABBV' 'ABMD' 'ACN' 'ATVI' 'ADM' 'ADBE' 'ADP' 'AAP'\n",
      " 'AES' 'AFL' 'A' 'AIG' 'APD' 'AKAM' 'ALK' 'ALB' 'ARE' 'ALGN' 'ALLE' 'LNT'\n",
      " 'ALL' 'GOOGL' 'GOOG' 'MO' 'AMZN' 'AMCR' 'AMD' 'AEE' 'AAL' 'AEP' 'AXP'\n",
      " 'AMT' 'AWK' 'AMP' 'ABC' 'AME' 'AMGN' 'APH' 'ADI' 'ANSS' 'ANTM' 'AON'\n",
      " 'APA' 'AAPL' 'AMAT' 'APTV' 'ANET' 'AIZ' 'T' 'ATO' 'ADSK' 'AZO' 'AVB'\n",
      " 'AVY' 'BKR' 'BALL' 'BAC' 'BBWI' 'BAX' 'BDX' 'WRB' 'BRK.B' 'BBY' 'BIO'\n",
      " 'TECH' 'BIIB' 'BLK' 'BK' 'BA' 'BKNG' 'BWA' 'BXP' 'BSX' 'BMY' 'AVGO' 'BR'\n",
      " 'BRO' 'BF.B' 'CHRW' 'CDNS' 'CZR' 'CPT' 'CPB' 'COF' 'CAH' 'KMX' 'CCL'\n",
      " 'CARR' 'CTLT' 'CAT' 'CBOE' 'CBRE' 'CDW' 'CE' 'CNC' 'CNP' 'CDAY' 'CERN'\n",
      " 'CF' 'CRL' 'SCHW' 'CHTR' 'CVX' 'CMG' 'CB' 'CHD' 'CI' 'CINF' 'CTAS' 'CSCO'\n",
      " 'C' 'CFG' 'CTXS' 'CLX' 'CME' 'CMS' 'KO' 'CTSH' 'CL' 'CMCSA' 'CMA' 'CAG'\n",
      " 'COP' 'ED' 'STZ' 'CEG' 'COO' 'CPRT' 'GLW' 'CTVA' 'COST' 'CTRA' 'CCI'\n",
      " 'CSX' 'CMI' 'CVS' 'DHI' 'DHR' 'DRI' 'DVA' 'DE' 'DAL' 'XRAY' 'DVN' 'DXCM'\n",
      " 'FANG' 'DLR' 'DFS' 'DISH' 'DIS' 'DG' 'DLTR' 'D' 'DPZ' 'DOV' 'DOW' 'DTE'\n",
      " 'DUK' 'DRE' 'DD' 'DXC' 'EMN' 'ETN' 'EBAY' 'ECL' 'EIX' 'EW' 'EA' 'EMR'\n",
      " 'ENPH' 'ETR' 'EOG' 'EPAM' 'EFX' 'EQIX' 'EQR' 'ESS' 'EL' 'ETSY' 'RE'\n",
      " 'EVRG' 'ES' 'EXC' 'EXPE' 'EXPD' 'EXR' 'XOM' 'FFIV' 'FDS' 'FAST' 'FRT'\n",
      " 'FDX' 'FITB' 'FRC' 'FE' 'FIS' 'FISV' 'FLT' 'FMC' 'F' 'FTNT' 'FTV' 'FBHS'\n",
      " 'FOXA' 'FOX' 'BEN' 'FCX' 'AJG' 'GRMN' 'IT' 'GE' 'GNRC' 'GD' 'GIS' 'GPC'\n",
      " 'GILD' 'GL' 'GPN' 'GM' 'GS' 'GWW' 'HAL' 'HIG' 'HAS' 'HCA' 'PEAK' 'HSIC'\n",
      " 'HSY' 'HES' 'HPE' 'HLT' 'HOLX' 'HD' 'HON' 'HRL' 'HST' 'HWM' 'HPQ' 'HUM'\n",
      " 'HII' 'HBAN' 'IEX' 'IDXX' 'ITW' 'ILMN' 'INCY' 'IR' 'INTC' 'ICE' 'IBM'\n",
      " 'IP' 'IPG' 'IFF' 'INTU' 'ISRG' 'IVZ' 'IPGP' 'IQV' 'IRM' 'JBHT' 'JKHY' 'J'\n",
      " 'JNJ' 'JCI' 'JPM' 'JNPR' 'K' 'KEY' 'KEYS' 'KMB' 'KIM' 'KMI' 'KLAC' 'KHC'\n",
      " 'KR' 'LHX' 'LH' 'LRCX' 'LW' 'LVS' 'LDOS' 'LEN' 'LLY' 'LNC' 'LIN' 'LYV'\n",
      " 'LKQ' 'LMT' 'L' 'LOW' 'LUMN' 'LYB' 'MTB' 'MRO' 'MPC' 'MKTX' 'MAR' 'MMC'\n",
      " 'MLM' 'MAS' 'MA' 'MTCH' 'MKC' 'MCD' 'MCK' 'MDT' 'MRK' 'FB' 'MET' 'MTD'\n",
      " 'MGM' 'MCHP' 'MU' 'MSFT' 'MAA' 'MRNA' 'MHK' 'MOH' 'TAP' 'MDLZ' 'MPWR'\n",
      " 'MNST' 'MCO' 'MS' 'MOS' 'MSI' 'MSCI' 'NDAQ' 'NTAP' 'NFLX' 'NWL' 'NEM'\n",
      " 'NWSA' 'NWS' 'NEE' 'NLSN' 'NKE' 'NI' 'NDSN' 'NSC' 'NTRS' 'NOC' 'NLOK'\n",
      " 'NCLH' 'NRG' 'NUE' 'NVDA' 'NVR' 'NXPI' 'ORLY' 'OXY' 'ODFL' 'OMC' 'OKE'\n",
      " 'ORCL' 'OGN' 'OTIS' 'PCAR' 'PKG' 'PARA' 'PH' 'PAYX' 'PAYC' 'PYPL' 'PENN'\n",
      " 'PNR' 'PEP' 'PKI' 'PFE' 'PM' 'PSX' 'PNW' 'PXD' 'PNC' 'POOL' 'PPG' 'PPL'\n",
      " 'PFG' 'PG' 'PGR' 'PLD' 'PRU' 'PEG' 'PTC' 'PSA' 'PHM' 'PVH' 'QRVO' 'PWR'\n",
      " 'QCOM' 'DGX' 'RL' 'RJF' 'RTX' 'O' 'REG' 'REGN' 'RF' 'RSG' 'RMD' 'RHI'\n",
      " 'ROK' 'ROL' 'ROP' 'ROST' 'RCL' 'SPGI' 'CRM' 'SBAC' 'SLB' 'STX' 'SEE'\n",
      " 'SRE' 'NOW' 'SHW' 'SBNY' 'SPG' 'SWKS' 'SJM' 'SNA' 'SEDG' 'SO' 'LUV' 'SWK'\n",
      " 'SBUX' 'STT' 'STE' 'SYK' 'SIVB' 'SYF' 'SNPS' 'SYY' 'TMUS' 'TROW' 'TTWO'\n",
      " 'TPR' 'TGT' 'TEL' 'TDY' 'TFX' 'TER' 'TSLA' 'TXN' 'TXT' 'TMO' 'TJX' 'TSCO'\n",
      " 'TT' 'TDG' 'TRV' 'TRMB' 'TFC' 'TWTR' 'TYL' 'TSN' 'USB' 'UDR' 'ULTA' 'UAA'\n",
      " 'UA' 'UNP' 'UAL' 'UNH' 'UPS' 'URI' 'UHS' 'VLO' 'VTR' 'VRSN' 'VRSK' 'VZ'\n",
      " 'VRTX' 'VFC' 'VTRS' 'V' 'VNO' 'VMC' 'WAB' 'WMT' 'WBA' 'WBD' 'WM' 'WAT'\n",
      " 'WEC' 'WFC' 'WELL' 'WST' 'WDC' 'WRK' 'WY' 'WHR' 'WMB' 'WTW' 'WYNN' 'XEL'\n",
      " 'XYL' 'YUM' 'ZBRA' 'ZBH' 'ZION' 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "# List of all S&P 500 tickers\n",
    "import pandas as pd\n",
    "table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "df_tickers = table[0]\n",
    "df_tickers.to_csv('S&P500-Info.csv')\n",
    "df_tickers.to_csv(\"S&P500-Symbols.csv\", columns=['Symbol'])\n",
    "tickers=df_tickers[\"Symbol\"].values\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e04f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867a170d1217447c8c448c1f518175b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for now scrapp all earning call transcript from one company\n",
    "for ticker in ['ABT']:\n",
    "#for ticker in tickers:\n",
    "#for ticker in ['MMM','AOS','ABT']:\n",
    "    html_path = \"data/ticker/\"+ticker ## todo save file with name time step*\n",
    "    last_time_scrap_path =html_path+ \"/last_time_scrap\"\n",
    "    if not os.path.exists(html_path):\n",
    "        os.makedirs(html_path)\n",
    "        print(f\"The new directory {html_path} has been created!\")\n",
    "    if os.path.exists(last_time_scrap_path):\n",
    "        with open(last_time_scrap_path, \"r\") as file:\n",
    "            publish_time_min = file.read()\n",
    "    else : \n",
    "        publish_time_min = None\n",
    "    if publish_time_min !=\"all\":\n",
    "        more_data = True\n",
    "        executable_path = \"./chromedriver\"\n",
    "        service = Service(executable_path)\n",
    "        option = webdriver.ChromeOptions()\n",
    "        driver = webdriver.Chrome(service=service,options=option)\n",
    "        id_URLs = []\n",
    "        id_to_date = {}\n",
    "        while more_data:\n",
    "            url = create_url_for_scrap(ticker, publish_time_min)\n",
    "            driver.get(url)\n",
    "            json_response = driver.find_element(by=By.TAG_NAME, value = \"body\").text\n",
    "            data_responses = json.loads(json_response)\n",
    "            if \"meta\" not in data_responses:\n",
    "                time.sleep(5) # 2 chances to be flag as robot\n",
    "                driver.get(url)\n",
    "                json_response = driver.find_element(by=By.TAG_NAME, value = \"body\").text\n",
    "                data_responses = json.loads(json_response)\n",
    "                if \"meta\" not in data_responses:\n",
    "                    last_publish_time_min = publish_time_min\n",
    "                    break\n",
    "            publish_time_min = str(data_responses[\"meta\"][\"page\"][\"minmaxPublishOn\"][\"min\"]) # need to be string\n",
    "            if publish_time_min!= 'None':\n",
    "                for element in data_responses[\"data\"]:\n",
    "                    if is_earning_call(element):\n",
    "                        URL, call_id, date = get_url_id_date_earning_call(element)\n",
    "                        id_URLs.append((call_id,URL))\n",
    "                        id_to_date[call_id] = date\n",
    "            else :\n",
    "                last_publish_time_min = \"all\"\n",
    "                more_data = False\n",
    "            time.sleep(2+random.uniform(0, 1))\n",
    "            \n",
    "    # Download HTML files\n",
    "    domain_name = 'https://seekingalpha.com'\n",
    "    for call_id, URL in tqdm(id_URLs):\n",
    "        file_path = html_path + \"/\"+ call_id + \".html\"\n",
    "        with open(file_path, \"w\") as file :\n",
    "            url = domain_name+URL\n",
    "            html =  requests.get(url).text\n",
    "            file.write(html)\n",
    "            \n",
    "    # Save the last scrapped url of the ticker, so that if bot detected can resume\n",
    "    with open(html_path+\"/last_time_scrap\", \"w\") as file:\n",
    "        file.write(last_publish_time_min)    \n",
    "    \n",
    "    # Check last file got downloaded, i.e. captcha/bot error\n",
    "    #BeautifulSoup(html).get_text()[:2000]\n",
    "    \n",
    "    # Save date of transcripts\n",
    "    id_to_date_path = \"data/ticker/id_to_date\"\n",
    "    if not os.path.exists(id_to_date_path):\n",
    "        file_id_to_date = {}\n",
    "    else:\n",
    "        with open(id_to_date_path,\"rb\") as file :\n",
    "            file_id_to_date = pickle.load(file)\n",
    "    for call_id, date in id_to_date.items():\n",
    "        file_id_to_date[call_id] = date\n",
    "\n",
    "    with open(id_to_date_path,\"wb\") as file : \n",
    "        pickle.dump(file_id_to_date, file, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ebb86",
   "metadata": {},
   "source": [
    "Download HTML files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0ddd2",
   "metadata": {},
   "source": [
    "domain_name = 'https://seekingalpha.com'\n",
    "for call_id, URL in tqdm(id_URLs):\n",
    "    file_path = html_path + \"/\"+ call_id + \".html\"\n",
    "    with open(file_path, \"w\") as file :\n",
    "        url = domain_name+URL\n",
    "        html =  requests.get(url).text\n",
    "        file.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edf351",
   "metadata": {},
   "source": [
    "Save the last scrapped url of the ticker, so that if bot detected can resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52798980",
   "metadata": {},
   "source": [
    "with open(html_path+\"/last_time_scrap\", \"w\") as file:\n",
    "    file.write(last_publish_time_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b0260",
   "metadata": {},
   "source": [
    "## Check last file got downloaded, i.e. captcha/bot error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe53a9",
   "metadata": {},
   "source": [
    "BeautifulSoup(html).get_text()[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aac81f",
   "metadata": {},
   "source": [
    "Save date of transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29bb34",
   "metadata": {},
   "source": [
    "id_to_date_path = \"data/ticker/id_to_date\"\n",
    "if not os.path.exists(id_to_date_path):\n",
    "    file_id_to_date = {}\n",
    "else:\n",
    "    with open(id_to_date_path,\"rb\") as file :\n",
    "        file_id_to_date = pickle.load(file)\n",
    "for call_id, date in id_to_date.items():\n",
    "    file_id_to_date[call_id] = date\n",
    "    \n",
    "with open(id_to_date_path,\"wb\") as file : \n",
    "    pickle.dump(file_id_to_date, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e133216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
