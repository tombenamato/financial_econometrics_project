{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a6c2ef",
   "metadata": {},
   "source": [
    "# Topic Modeling \n",
    "This notebook aims to take as input the texts who have been processed and use it to find the most relevants topics and the words that are used for the sentimental analysis.\n",
    "\n",
    "**Implementation**\n",
    "- TF-IDF\n",
    "- FinBERT\n",
    "- LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea88cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import pre_processing_final as p\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from finbert_embedding.embedding import FinbertEmbedding\n",
    "\n",
    "import hdbscan\n",
    "import gensim\n",
    "import finbert_embedding\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.corpora.dictionary import Dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd213fc",
   "metadata": {},
   "source": [
    "### Import and process the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf44387",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, articles = p.import_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012efff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed = [p.process_figas(t,'aa') for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b7f4",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87daa161",
   "metadata": {},
   "source": [
    "Take the top n words depending on the score with the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_TFIDF_words(n, texts_processed):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(texts_processed)\n",
    "    print(X_tfidf.shape)\n",
    "\n",
    "    # Create dictionnary with all the words contained in the TF-IDF matrix\n",
    "    dict_w_index = vectorizer.vocabulary_\n",
    "    dict_index_w = {v: k for k, v in dict_w_index.items()}\n",
    "    \n",
    "    n =10\n",
    "    top_n = []\n",
    "    for i in range(X_tfidf.shape[0]):\n",
    "        index = X_tfidf[i,].nonzero()[1]\n",
    "        words_of_index = [dict_index_w[x] for x in index]\n",
    "        score_of_index = [X_tfidf[i,x] for x in index]\n",
    "        x = list(zip(words_of_index,score_of_index))\n",
    "        x.sort(key=lambda x: -x[1])\n",
    "        a = [w[0] for w in x[:n]]\n",
    "        top_n.append(a)\n",
    "    \n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_top_n_words = top_n_TFIDF_words(10, texts_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6699b",
   "metadata": {},
   "source": [
    "Save the top words found with TFIDF in a pickle with the article IDm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'article':articles,'top_n_words':tfidf_top_n_words})\n",
    "df.to_pickle(\"data/top_n_words_tfidf_proc_figass_with_juliette_geneve.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(texts_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbd5e6",
   "metadata": {},
   "source": [
    "### FinBERT \n",
    "https://pypi.org/project/finbert-embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10848b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = FinbertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fffa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FinB = np.zeros((len(texts_processed),768))\n",
    "k=0\n",
    "for text in texts:\n",
    "    X_FinB[k,] = finbert.sentence_vector(text)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnary with all the words contained in the TF-IDF matrix\n",
    "    dict_w_index = vectorizer.vocabulary_\n",
    "    dict_index_w = {v: k for k, v in dict_w_index.items()}\n",
    "    \n",
    "    n =10\n",
    "    top_n = []\n",
    "    for i in range(X_tfidf.shape[0]):\n",
    "        index = X_tfidf[i,].nonzero()[1]\n",
    "        words_of_index = [dict_index_w[x] for x in index]\n",
    "        score_of_index = [X_tfidf[i,x] for x in index]\n",
    "        x = list(zip(words_of_index,score_of_index))\n",
    "        x.sort(key=lambda x: -x[1])\n",
    "        a = [w[0] for w in x[:n]]\n",
    "        top_n.append(a)\n",
    "    \n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FinB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0d92d",
   "metadata": {},
   "source": [
    "### Dimensionality reduction and clustering before topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc815fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = X_tfidf\n",
    "reducer = umap.UMAP()\n",
    "umap_embeddings = umap.UMAP(n_components= 15, n_neighbors=15, metric='cosine').fit_transform(embeddings)\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=5,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
    "\n",
    "print('Number of clusters/topics ', len(set(cluster.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2376ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.5)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=5, cmap='hsv_r')\n",
    "plt.savefig('clusters.png')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b828eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(texts, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1912944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m):\n",
    "    count = CountVectorizer(stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(texts_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df)\n",
    "topic_sizes = topic_sizes[topic_sizes.Size >=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for t in topic_sizes.Topic.values:\n",
    "    if(t!=-1):\n",
    "        top_n_words[t].sort(key=lambda x:- x[1])\n",
    "        k +=1\n",
    "        print('')\n",
    "        print('Topic - ',k)\n",
    "        for i in range(10):\n",
    "            print(top_n_words[t][i][0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8afb20c03eb43ee960a6eb9b47a880f3e5ef6fa8a91dccdd9bd06197b17763f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
