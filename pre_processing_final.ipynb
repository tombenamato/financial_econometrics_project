{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec66965a",
   "metadata": {},
   "source": [
    "# Processing\n",
    "This notebooks contains all the functions needed to download all the texts and also process them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c61dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09874bea",
   "metadata": {},
   "source": [
    "## Import the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_texts():\n",
    "    \"\"\"Function that will find all the earning calls downloaded and import them. It returns a list with all the texts.\"\"\"\n",
    "    dir_path = \"data/text/\"\n",
    "    list_tickers = os.listdir(dir_path)\n",
    "    texts = []\n",
    "    articles = []\n",
    "    compagny = []\n",
    "    for ticker in list_tickers:\n",
    "        earning_call_path = \"data/text/\"+ticker\n",
    "        list_articles = os.listdir(earning_call_path)\n",
    "        \n",
    "        list_articles = [earning_call_path+'/'+x for x in list_articles if x!='.DS_Store' and x!= '.ipynb_checkpoints']\n",
    "        for s in list_articles:\n",
    "            with open(s) as f:\n",
    "                x = int(re.sub(earning_call_path+'/','',s))\n",
    "                articles.append(x)\n",
    "                t = f.read()\n",
    "                texts.append(t)\n",
    "                compagny.append(ticker)\n",
    "        \n",
    "    print('Number of articles', len(texts))\n",
    "    \n",
    "    df = pd.DataFrame({'ticker':compagny,'article':articles,'text':texts})\n",
    "    df.text = df.text.apply(lambda x : re.sub('Question-and-Answer Session','',x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f454ad1",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_text(list_input, stops = []):\n",
    "    \"Function that take as input a text that have been tokenized and put it back into one single string\"\n",
    "    text_output = ' '.join([word for word in list_input if word not in stops]) \n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list(text_input):\n",
    "    \"\"\"Function that takes as input a text and tokenize it\"\"\"\n",
    "    list_output = word_tokenize(text_input)\n",
    "    return list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_list(n):\n",
    "    \"\"\"Function that takes as input n which correspond to the blocks that we want to do with n-grams\"\"\"\n",
    "    m = []\n",
    "    nx_grams = ngrams(sequence = nltk.word_tokenize(text), n = n)\n",
    "    for gram in nx_grams:\n",
    "        m.append(gram)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(filtered_text, a, l, u):\n",
    "    \"\"\"Remove most and least frequent words with a given lower (l) and upper (u) bound to remove certain percentage of occurences\"\"\"\n",
    "    f = FreqDist(a)\n",
    "\n",
    "    df_fdist = pd.DataFrame({'Word': f.keys(), 'Number of apparitions': f.values()})\n",
    "    L= l*len(df_fdist)\n",
    "    L=int(L)\n",
    "\n",
    "    H=u*len(df_fdist)\n",
    "    H=int(H)\n",
    "    \n",
    "    df_fdesc = df_fdist.sort_values(by='Number of apparitions', ascending=False)\n",
    "    df_fasc = df_fdist.sort_values(by='Number of apparitions', ascending=True)\n",
    "\n",
    "    most_freq_words_list = list(df_fdesc['Word'][:H])\n",
    "    least_freq_word_list = list(df_fasc['Word'][:L])\n",
    "    stopwords = most_freq_words_list + least_freq_word_list\n",
    "    textlist_wo_extremes = list_to_text(filtered_text, stopwords)\n",
    "\n",
    "    return textlist_wo_extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_speakers(text):\n",
    "    \"\"\"Function ta remove the speakers from one text\"\"\"\n",
    "    sentences = []\n",
    "    list_string = text.split('\\n \\n')\n",
    "    for s in list_string:\n",
    "        if(len(s.split(' '))>3):\n",
    "            sentences.append(s)\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536579e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processing(text, lower_bound, upper_bound, noun):\n",
    "    \"\"\"Function that combine all the processing steps\"\"\"\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    x = [WordNetLemmatizer.lemmatize(word, 'n') for word in filtered_text]\n",
    "    filter1 = remove(filtered_text, x ,lower_bound, upper_bound)\n",
    "    if(noun):\n",
    "        return remove_speakers(filter1)\n",
    "    else: \n",
    "        return filter1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c624d58",
   "metadata": {},
   "source": [
    "### Processing FiGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d78597",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def sentences_process(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def remove_most_least_freq(list_words, lower, upper):\n",
    "    \n",
    "    f = FreqDist(list_words)\n",
    "    df_fdist = pd.DataFrame({'Word': f.keys(), 'Number of apparitions': f.values()})\n",
    "    num_lower=int(lower*len(df_fdist))\n",
    "    num_upper=int(upper*len(df_fdist))\n",
    "    \n",
    "    vocabulary = Counter(list_words)\n",
    "    sorted_vocabulary = sorted(vocabulary.items(), key=lambda x: x[1])\n",
    "    most_common = sorted_vocabulary[-num_upper:][::-1]\n",
    "    least_common = sorted_vocabulary[:num_lower]\n",
    "    stopwords = [x[0] for x in most_common+least_common]\n",
    "    \n",
    "    return list_to_text(list_words, stopwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_filter(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    words = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in  ['LOC','GPE','PERSON']:\n",
    "            words.append('JULIETTE')\n",
    "\n",
    "    return list_to_text(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c26399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_figas(text):\n",
    "    filter_speaker = remove_speakers(text)\n",
    "    filter_char = sentences_process(filter_speaker)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(filter_char)\n",
    "    filter_stopwords = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    filter_freq = remove_most_least_freq(filter_stopwords, 0.06, 0.06)\n",
    "    \n",
    "    tokenization = word_tokenize(filter_freq)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatization = ' '.join([lemmatizer.lemmatize(w) for w in tokenization])\n",
    "    \n",
    "    return lemmatization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32316521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8874bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df.text.apply(lambda x: process_figas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['l'] = df.text_processed.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.l==0].article.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_process_0 = [3440166, 4358280, 2343215, 4387002,  320064, 4387940, 2600265,\n",
    "       2375305,  729441,  572641, 3834286, 3965900, 3074656, 2144593,\n",
    "       4012772,  321956,  234463, 4342792]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.l>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a564dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_TFIDF_words(n, texts_processed):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(texts_processed)\n",
    "\n",
    "    # Create dictionnary with all the words contained in the TF-IDF matrix\n",
    "    dict_w_index = vectorizer.vocabulary_\n",
    "    dict_index_w = {v: k for k, v in dict_w_index.items()}\n",
    "    \n",
    "    top_n = []\n",
    "    for i in range(X_tfidf.shape[0]):\n",
    "        index = X_tfidf[i,].nonzero()[1]\n",
    "        words_of_index = [dict_index_w[x] for x in index]\n",
    "        score_of_index = [X_tfidf[i,x] for x in index]\n",
    "        x = list(zip(words_of_index,score_of_index))\n",
    "        x.sort(key=lambda x: -x[1])\n",
    "        a = [w[0] for w in x[:n]]\n",
    "        top_n.append(a)\n",
    "    \n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18203e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tfidf_top_10_words'] = top_n_TFIDF_words(10, df.text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle = pd.DataFrame({'ticker':df.ticker,'article':df.article,'top_n_words':df.tfidf_top_10_words})\n",
    "df_pickle.to_pickle(\"data/sp500_top_10_words_tfidf_proc_figass_by_corpus.pkl\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f475da",
   "metadata": {},
   "source": [
    "### TF-IDF BY TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_speakers2(text):\n",
    "    \"\"\"Function ta remove the speakers from one text\"\"\"\n",
    "    sentences = []\n",
    "    list_string = text.split('\\n \\n')\n",
    "    for s in list_string:\n",
    "        if(len(s.split(' '))>3):\n",
    "            sentences.append(s)\n",
    "    return '||'.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd69487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_figas2(text):\n",
    "    filter_speaker = remove_speakers2(text)\n",
    "    filter_char = sentences_process(filter_speaker)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(filter_char)\n",
    "    filter_stopwords = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    filter_freq = remove_most_least_freq(filter_stopwords, 0.06, 0.06)\n",
    "    \n",
    "    tokenization = word_tokenize(filter_freq)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatization = ' '.join([lemmatizer.lemmatize(w) for w in tokenization])\n",
    "    \n",
    "    return lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_by_text(text):\n",
    "    list_sentences = text.split('||')\n",
    "    out = ['.'.join(list_sentences[k:k+2]) for k in range(0, len(list_sentences), 2)]\n",
    "    top_3_words = top_n_TFIDF_words(3, out)\n",
    "    top_words = set(sum(top_3_words, []))\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b253b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_by_text = df[['ticker','article','text']]\n",
    "df_tfidf_by_text['text_processed'] = df_tfidf_by_text.text.apply(lambda x : process_figas2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_by_text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048cb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_by_text['top_10_words'] = df_tfidf_by_text.text_processed.apply(lambda x :tfidf_by_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_by_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76aa7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle2 = pd.DataFrame({'ticker':df_tfidf_by_text.ticker,'article':df_tfidf_by_text.article,'top_n_words':df_tfidf_by_text.top_10_words})\n",
    "df_pickle2.to_pickle(\"data/sp500_top_10_words_tfidf_proc_figass_by_text.pkl\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623bcc0",
   "metadata": {},
   "source": [
    "### Name Entity Recognition\n",
    "\n",
    "- python -m spacy download en_core_web_lg\n",
    "- python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "SpaCy:\n",
    "- **PERSON**:      People, including fictional.\n",
    "- NORP:        Nationalities or religious or political groups.\n",
    "- FAC:         Buildings, airports, highways, bridges, etc.\n",
    "- **ORG**:         Companies, agencies, institutions, etc.\n",
    "- GPE:         Countries, cities, states.\n",
    "- **LOC**:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LAW:         Named documents made into laws.\n",
    "- LANGUAGE:    Any named language.\n",
    "- **DATE**:        Absolute or relative dates or periods.\n",
    "- **TIME**:        Times smaller than a day.\n",
    "- PERCENT:     Percentage, including ”%“.\n",
    "- MONEY:       Monetary values, including unit.\n",
    "- QUANTITY:    Measurements, as of weight or distance.\n",
    "- ORDINAL:     “first”, “second”, etc.\n",
    "- CARDINAL:    Numerals that do not fall under another type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "dictionary=Counter(tokens_)\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Read the whole text.\n",
    "wordcloud = WordCloud(width=1600, height=800, colormap=\"hsv\", background_color='white').generate_from_frequencies(dictionary)\n",
    "# Open a plot of the generated image.\n",
    "\n",
    "plt.figure( figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('imagedist.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8afb20c03eb43ee960a6eb9b47a880f3e5ef6fa8a91dccdd9bd06197b17763f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
