{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b212840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from finbert_embedding.embedding import FinbertEmbedding\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os.path\n",
    "from pandas import *\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098b0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data_directory = os.path.abspath(\"./data/sp500\")\n",
    "path_text = current_data_directory + \"/text/\"\n",
    "path_to_id_csv = current_data_directory + \"/df_100stocks_sp500.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ac7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/juliencyrusenyegue/Documents/GitHub/financial_econometrics_project/FinBert/data/sp500\n"
     ]
    }
   ],
   "source": [
    "print(current_data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e815744",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(path_to_id_csv)\n",
    "ID_sp500 = data['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe717c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad0b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fa6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts from the directory\n",
    "\n",
    "def extract_texts():\n",
    "\n",
    "    texts = []\n",
    "    first_sentence = []\n",
    "    articles = []\n",
    "    list_ID = []\n",
    "   \n",
    "    list_tickers = os.listdir(path_text)\n",
    "    #list_tickers.remove('.DS_Store') # for mac\n",
    "    \n",
    "    for ticker in list_tickers:\n",
    "        \n",
    "        list_articles = glob.glob(path_text+ticker+\"/*\")\n",
    "        \n",
    "        ID = []\n",
    "        for s in list_articles:\n",
    "            \n",
    "            test_id = int(s.replace(path_text+str(ticker)+\"/\", \"\"))\n",
    "            \n",
    "            \n",
    "            if test_id in ID_sp500:\n",
    "                \n",
    "                with open(s) as f:\n",
    "                    \n",
    "                    x = re.sub(path_text+str(ticker)+\"/\",\"\",s)\n",
    "                    articles.append(x)\n",
    "                    t = f.read()\n",
    "                    texts.append(t)\n",
    "        \n",
    "                ID.append(test_id)\n",
    "                  \n",
    "        list_ID += ID\n",
    "    \n",
    "    print(len(texts))\n",
    "     \n",
    "    return texts, list_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affa361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "    #sentences = [x for x in sentences if len(x.split(' '))>5]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad5fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run finbert on each text and count the nb of neutral, positive, negative sentences\n",
    "\n",
    "def finbert_texts(start,n_texts, texts):\n",
    "    labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "    data = np.zeros((n_texts,3))\n",
    "    \n",
    "    for j in tqdm(range(start,start + n_texts)):\n",
    "        \n",
    "        print(j - start + 1)\n",
    "        #sentences = split_into_sentences(texts[j])\n",
    "        sentences = texts[j].split('\\n \\n')\n",
    "        sentences = [x for x in sentences if len(x.split(' '))>4]\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True,truncation=True,max_length=512)\n",
    "\n",
    "\n",
    "        outputs = finbert(**inputs)[0]\n",
    "        #print(outputs)\n",
    "        i = np.argmax(outputs.detach().numpy(), axis = 1)\n",
    "        #print(i)\n",
    "        sentiment_counter = list(Counter(i).values())\n",
    "        if(np.shape(sentiment_counter) == (2, )):\n",
    "            sentiment_counter.append(0)\n",
    "        print(sentiment_counter)\n",
    "        data[j-start,:] = sentiment_counter\n",
    "        \n",
    "        time.sleep(45)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7269ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'articles\n",
    "N = 98\n",
    "start = 302"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16973d",
   "metadata": {},
   "source": [
    "li = [1,2]\n",
    "#print(np.shape(li))\n",
    "\n",
    "if(np.shape(li) == (2, )):\n",
    "    li.append(4)\n",
    "    print(li)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43047917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[36, 40, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                         | 1/98 [01:13<1:58:34, 73.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[58, 25, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                         | 2/98 [03:10<2:38:47, 99.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[54, 29, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                       | 3/98 [05:18<2:57:40, 112.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    " if __name__ == \"__main__\":\n",
    "    \n",
    "    texts, list_ID = extract_texts()\n",
    "    data = finbert_texts(start, N, texts)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratios computations\n",
    "\n",
    "df_data = pd.DataFrame(data, columns= [\"Neutral\",\"Positive\", \"Negative\"])\n",
    "\n",
    "\n",
    "df_data[\"Total\"] = df_data[\"Neutral\"] + df_data[\"Positive\"] + df_data[\"Negative\"]\n",
    "df_data[\"Difference\"] = df_data[\"Positive\"] - df_data[\"Negative\"]\n",
    "df_data[\"P_N ratio\"] = df_data[\"Positive\"] / df_data[\"Negative\"]\n",
    "df_data[\"Difference ratio\"] = df_data[\"Difference\"] / df_data[\"Total\"]\n",
    "\n",
    "df_data[\"Decile\"] = 1 + df_data[\"Difference ratio\"].transform(lambda y: pd.qcut(y, 10, labels=False))\n",
    "\n",
    "df_data = np.round(df_data, 5)\n",
    "\n",
    "\n",
    "df_data[\"ID\"] = list_ID[start: start + N]\n",
    "first_column = df_data.pop(\"ID\")\n",
    "df_data.insert(0, \"ID\", first_column)\n",
    "\n",
    "#long = list(df_data.loc[df_data['Decile'].isin([10,9])][\"ID\"])\n",
    "#short = list(df_data.loc[df_data['Decile'].isin([1,2])][\"ID\"])\n",
    "\n",
    "display(df_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba615869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas run à la première itération\n",
    "\n",
    "df_ratios = read_csv(os.path.abspath(\"./FinBert_ratios.csv\"))\n",
    "display(df_ratios)\n",
    "df_ratios = pd.concat([df_ratios, df_data])\n",
    "df_ratios[\"Decile\"] = 1 + df_ratios[\"Difference ratio\"].transform(lambda y: pd.qcut(y, 10, labels=False))\n",
    "display(df_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratios.to_csv('FinBert_ratios.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5fe50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
