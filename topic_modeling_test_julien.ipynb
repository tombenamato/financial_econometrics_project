{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a6c2ef",
   "metadata": {},
   "source": [
    "# Topic Modeling \n",
    "This notebook aims to take as input the texts who have been processed and use it to find the most relevants topics and the words that are relevant for the sentimental analysis.\n",
    "\n",
    "**Implementation**\n",
    "- TF-IDF\n",
    "- FinBERT\n",
    "- LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea88cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from pre_processing.ipynb\n",
      "['4509509', '4508877', '4509199', '4507655', '4509360', '4508080', '4509536', '4508410', '4508622', '4509394', '4508848', '4508074', '4508428', '4508870', '4509358', '4508884', '4509393', '4508625', '4509367', '4508613', '4508421', '4508879', '4508045', '4509163', '4509507', '4509369', '4507697', '4508284', '4509356', '4509164', '4508042', '4509190', '4508486', '4509302', '4508016', '4508472', '4507637', '4508815', '4508475', '4508647', '4508223', '4509305', '4509137', '4509553', '4508481', '4508812', '4508678', '4507630', '4509108', '4508443', '4508685', '4509333', '4509101', '4508018', '4508824', '4508488', '4509334', '4508682', '4508676', '4508444', '4508020', '4509139', '4508823', '4508840', '4509368', '4508615', '4508285', '4509357', '4508847', '4508249', '4509350', '4508612', '4508878', '4508885', '4509359', '4509530', '4509366', '4509154', '4508086', '4509392', '4508072', '4508416', '4508624', '4509198', '4508876', '4507654', '4508882', '4508623', '4508849', '4509395', '4509153', '4508445', '4508021', '4509335', '4508683', '4508822', '4508648', '4508684', '4509332', '4508670', '4508442', '4508489', '4509304', '4508480', '4508474', '4509109', '4507631', '4508679', '4508225', '4508473', '4508641', '4508487', '4509303', '4508814', '4508028', '4509088', '4508368', '4509271', '4508191', '4508501', '4508733', '4509285', '4508539', '4507929', '4508162', '4508350', '4509282', '4508506', '4508196', '4508702', '4508530', '4508154', '4508392', '4509416', '4509278', '4508508', '4507918', '4508395', '4508361', '4508537', '4508705', '4507719', '4508751', '4508563', '4508904', '4508138', '4508564', '4508756', '4508100', '4509214', '4508590', '4508903', '4507721', '4508769', '4508304', '4507942', '4508136', '4508552', '4508760', '4508794', '4509222', '4508109', '4508793', '4508767', '4508555', '4508303', '4508509', '4508199', '4509080', '4508152', '4507926', '4508704', '4508536', '4509246', '4508394', '4508393', '4508531', '4508703', '4508367', '4508538', '4509277', '4508197', '4508351', '4509283', '4508507', '4508735', '4509089', '4507745', '4508732', '4508500', '4508190', '4508554', '4508766', '4507944', '4508130', '4509224', '4508792', '4508795', '4509011', '4508137', '4507943', '4508761', '4508553', '4508598', '4509215', '4508591', '4508757', '4508565', '4508101', '4507972', '4508106', '4508562', '4507718', '4507986', '4508139', '4508329', '4508772', '4508540', '4508578', '4507968', '4508920', '4508311', '4508547', '4508775', '4509201', '4507995', '4508585', '4508571', '4508743', '4508549', '4507959', '4507966', '4508320', '4508744', '4508576', '4508582', '4508374', '4509094', '4507932', '4508146', '4508522', '4508710', '4509404', '4508380', '4509299', '4509255', '4508387', '4508717', '4508525', '4508141', '4507935', '4509297', '4508513', '4508514', '4508342', '4509290', '4509264', '4508719', '4508548', '4507958', '4508583', '4507993', '4508577', '4508745', '4508789', '4508742', '4508114', '4507960', '4508584', '4509456', '4508579', '4509209', '4507956', '4508774', '4508546', '4508780', '4508926', '4509231', '4508787', '4508541', '4508773', '4508317', '4509265', '4508515', '4508343', '4509291', '4508718', '4509296', '4508512', '4508720', '4509262', '4509050', '4508149', '4508524', '4508716', '4509092', '4507934', '4508140', '4509402', '4508729', '4509405', '4509095', '4508147', '4507933', '4508711', '4508523', '4509298', '4508836', '4508663', '4508451', '4508207', '4509321', '4508697', '4509319', '4508831', '4508690', '4509326', '4508200', '4508456', '4508664', '4508699', '4509579', '4508807', '4507625', '4509310', '4509122', '4508494', '4508460', '4508652', '4508838', '4508458', '4508800', '4509328', '4508231', '4508655', '4508467', '4508493', '4509317', '4509185', '4508601', '4509515', '4509171', '4507676', '4508854', '4509388', '4507682', '4509176', '4508262', '4507685', '4508853', '4508639', '4509524', '4508092', '4509372', '4508066', '4509386', '4508630', '4507678', '4508402', '4508405', '4509381', '4508253', '4509375', '4508095', '4508862', '4508608', '4509178', '4509329', '4508459', '4508801', '4508492', '4508466', '4508654', '4508806', '4507624', '4508839', '4508237', '4509311', '4508495', '4508468', '4508830', '4509318', '4508201', '4508665', '4508457', '4508691', '4509571', '4509327', '4508837', '4509320', '4508696', '4508450', '4508662', '4508206', '4508808', '4509146', '4509374', '4508404', '4509380', '4508897', '4508863', '4508609', '4508255', '4508067', '4508403', '4507679', '4508631', '4509373', '4508435', '4508607', '4508051', '4509183', '4509345', '4509177', '4508852', '4507670', '4508638', '4509148', '4509170', '4508290', '4509342', '4509184', '4508264', '4508600', '4508432', '4507683', '4507677', '4508855', '4508476', '4508012', '4509306', '4508482', '4508449', '4507633', '4509339', '4509133', '4508227', '4508829', '4508471', '4508688', '4507634', '4508816', '4508218', '4509105', '4508447', '4508023', '4509308', '4508820', '4508478', '4508818', '4508686', '4508229', '4508827', '4509559', '4508873', '4508887', '4509390', '4508070', '4508626', '4509364', '4508874', '4507656', '4508083', '4509151', '4507669', '4508621', '4508077', '4509397', '4507694', '4508628', '4509167', '4509355', '4509193', '4508425', '4508845', '4509399', '4507667', '4507693', '4508422', '4507658', '4508610', '4509194', '4509352', '4508280', '4509160', '4508687', '4509331', '4508819', '4508826', '4508674', '4508446', '4509336', '4509560', '4508821', '4508479', '4509309', '4508828', '4508014', '4508226', '4508470', '4508484', '4509300', '4508817', '4508689', '4509307', '4508483', '4508477', '4508645', '4508013', '4509338', '4508810', '4507632', '4508448', '4509398', '4508078', '4508844', '4507666', '4509161', '4509353', '4509505', '4507659', '4508423', '4509195', '4508629', '4509192', '4508875', '4507657', '4508412', '4508620', '4509396', '4508076', '4509362', '4508082', '4509150', '4508288', '4508872', '4509365', '4509391', '4508627', '4508755', '4508567', '4508103', '4508331', '4508593', '4507948', '4508558', '4507722', '4509228', '4507984', '4508104', '4507970', '4508560', '4508752', '4508799', '4508907', '4508309', '4508790', '4508556', '4508764', '4509219', '4507713', '4508569', '4507941', '4508307', '4508763', '4508551', '4508797', '4509221', '4508338', '4508708', '4507740', '4509078', '4508398', '4509281', '4508353', '4508161', '4508505', '4508737', '4509275', '4508195', '4508159', '4508192', '4508730', '4508502', '4509286', '4509049', '4508739', '4509076', '4507924', '4508534', '4508533', '4507749', '4508365', '4507923', '4508391', '4508796', '4509012', '4508134', '4508550', '4508762', '4508339', '4508765', '4508557', '4507947', '4508133', '4509227', '4508791', '4508568', '4508105', '4508939', '4508337', '4508753', '4508561', '4508595', '4509211', '4508308', '4508566', '4508754', '4508102', '4507949', '4507723', '4508559', '4509289', '4509070', '4508700', '4508532', '4508364', '4508738', '4508151', '4507925', '4508535', '4508707', '4509413', '4508397', '4508503', '4508731', '4509273', '4508193', '4508399', '4509079', '4508709', '4508194', '4509274', '4509280', '4508352', '4508736', '4508504', '4509400', '4508526', '4508142', '4508189', '4509269', '4508519', '4507931', '4508145', '4509097', '4508713', '4508521', '4509407', '4508383', '4508517', '4508173', '4509293', '4509055', '4508187', '4508528', '4509260', '4509294', '4508346', '4508948', '4508510', '4508749', '4507701', '4508312', '4508776', '4508544', '4508588', '4508543', '4507953', '4509233', '4508785', '4508778', '4508575', '4508747', '4508581', '4508586', '4508740', '4508572', '4507962', '4508324', '4508949', '4509295', '4508347', '4508511', '4509053', '4507754', '4508378', '4509408', '4509054', '4509430', '4508516', '4508724', '4508529', '4507930', '4508520', '4508947', '4508527', '4509401', '4508518', '4508188', '4508573', '4508741', '4508325', '4508779', '4508580', '4507990', '4507964', '4508110', '4508322', '4508746', '4508574', '4508589', '4509464', '4508770', '4508542', '4508126', '4508748', '4508121', '4508545', '4508777', '4508783', '4509347', '4508295', '4509175', '4508437', '4508605', '4508261', '4509181', '4507686', '4508850', '4507672', '4509186', '4508868', '4508430', '4509340', '4508292', '4507675', '4508259', '4508857', '4507681', '4508634', '4508406', '4509376', '4509520', '4508861', '4509527', '4509371', '4509143', '4508257', '4509385', '4508401', '4508892', '4509188', '4508866', '4508658', '4508832', '4508693', '4509117', '4509325', '4508203', '4508455', '4508835', '4508499', '4508452', '4507628', '4508660', '4508204', '4507842', '4509322', '4508694', '4508669', '4508803', '4509119', '4508232', '4508464', '4508656', '4508490', '4509126', '4508804', '4507626', '4509313', '4508497', '4508858', '4509384', '4508632', '4509526', '4509142', '4509377', '4509145', '4509521', '4508407', '4508635', '4509383', '4509348', '4508860', '4507642', '4509341', '4508431', '4508603', '4507680', '4509528', '4508856', '4508436', '4509174', '4508294', '4509510', '4509379', '4508099', '4508805', '4507627', '4508462', '4509312', '4508802', '4508491', '4509315', '4508233', '4508657', '4508465', '4508498', '4508834', '4509323', '4508695', '4509575', '4508661', '4507629', '4508453', '4508659', '4508833', '4508202', '4508454', '4509324']\n",
      "Number of articles 841\n",
      "Number of words before removing the stop words 0\n",
      "Number of words after removing the stop words 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliencyrusenyegue/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juliencyrusenyegue/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/juliencyrusenyegue/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/juliencyrusenyegue/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6a5d1c11ef445fadb1b9f6d9ebe8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "from pre_processing import processing\n",
    "from finbert_embedding.embedding import FinbertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd213fc",
   "metadata": {},
   "source": [
    "### Import the text and process it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed43e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'articles\n",
    "\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b599e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles 20\n"
     ]
    }
   ],
   "source": [
    "list_articles = glob.glob(\"data/earning_call/*\")[:N]\n",
    "texts = []\n",
    "first_sentence = []\n",
    "articles = []\n",
    "for s in list_articles:\n",
    "    with open(s) as f:\n",
    "        x = int(re.sub('data/earning_call/','',s))\n",
    "        articles.append(x)\n",
    "        t = f.read()\n",
    "        texts.append(t)\n",
    "        \n",
    "       \n",
    "        \n",
    "print('Number of articles', len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf44387",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [processing(x) for x in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b7f4",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1f7910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 6533)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fb68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_w_index = vectorizer.vocabulary_\n",
    "dict_index_w = {v: k for k, v in dict_w_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87daa161",
   "metadata": {},
   "source": [
    "Take the top n words depending on the score with the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a102efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =10\n",
    "top_n = []\n",
    "for i in range(X_tfidf.shape[0]):\n",
    "    index = X_tfidf[i,].nonzero()[1]\n",
    "    words_of_index = [dict_index_w[x] for x in index]\n",
    "    score_of_index = [X_tfidf[i,x] for x in index]\n",
    "    x = list(zip(words_of_index,score_of_index))\n",
    "    x.sort(key=lambda x: -x[1])\n",
    "    a = [w[0] for w in x[:n]]\n",
    "    top_n.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d02a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'article':articles,'file_path':list_articles,'top_n_words':top_n})\n",
    "df.to_pickle(\"data/top_n_words_tfidf.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3450ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>file_path</th>\n",
       "      <th>top_n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4509509</td>\n",
       "      <td>data/earning_call/4509509</td>\n",
       "      <td>[savings, hardware, app, 3pl, cogs, us, issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4508877</td>\n",
       "      <td>data/earning_call/4508877</td>\n",
       "      <td>[oneweb, chris, us, government, henry, caleb, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4509199</td>\n",
       "      <td>data/earning_call/4509199</td>\n",
       "      <td>[patients, us, oxybates, nda, avadel, ag, sodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4507655</td>\n",
       "      <td>data/earning_call/4507655</td>\n",
       "      <td>[customers, erik, aws, eps, capabilities, chad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4509360</td>\n",
       "      <td>data/earning_call/4509360</td>\n",
       "      <td>[brooks, notes, clinicians, customers, us, all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4508080</td>\n",
       "      <td>data/earning_call/4508080</td>\n",
       "      <td>[ounces, aris, amounted, trevor, haytham, rose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4509536</td>\n",
       "      <td>data/earning_call/4509536</td>\n",
       "      <td>[merchants, originations, cupito, consumers, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4508410</td>\n",
       "      <td>data/earning_call/4508410</td>\n",
       "      <td>[us, patients, valves, centers, physicians, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4508622</td>\n",
       "      <td>data/earning_call/4508622</td>\n",
       "      <td>[sales, increases, heppenstall, constant, impl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4509394</td>\n",
       "      <td>data/earning_call/4509394</td>\n",
       "      <td>[events, customers, planners, hotels, operator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4508848</td>\n",
       "      <td>data/earning_call/4508848</td>\n",
       "      <td>[ounces, recoveries, del, fruta, ore, explorat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4508074</td>\n",
       "      <td>data/earning_call/4508074</td>\n",
       "      <td>[burrows, customers, alberta, northeast, kwan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4508428</td>\n",
       "      <td>data/earning_call/4508428</td>\n",
       "      <td>[flows, advisors, seed, clients, pension, mcin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4508870</td>\n",
       "      <td>data/earning_call/4508870</td>\n",
       "      <td>[customers, operators, sutton, yoke, ravi, mik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4509358</td>\n",
       "      <td>data/earning_call/4509358</td>\n",
       "      <td>[patients, mutations, pathways, populations, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4508884</td>\n",
       "      <td>data/earning_call/4508884</td>\n",
       "      <td>[sales, doctors, patients, practices, procedur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4509393</td>\n",
       "      <td>data/earning_call/4509393</td>\n",
       "      <td>[proteins, modifications, kyle, molecule, tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4508625</td>\n",
       "      <td>data/earning_call/4508625</td>\n",
       "      <td>[surgeons, patients, procedures, sales, adduct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4509367</td>\n",
       "      <td>data/earning_call/4509367</td>\n",
       "      <td>[patients, tfi, year, responder, otonomy, call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4508613</td>\n",
       "      <td>data/earning_call/4508613</td>\n",
       "      <td>[clinics, openings, rd, owned, comps, franchis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article                  file_path  \\\n",
       "0   4509509  data/earning_call/4509509   \n",
       "1   4508877  data/earning_call/4508877   \n",
       "2   4509199  data/earning_call/4509199   \n",
       "3   4507655  data/earning_call/4507655   \n",
       "4   4509360  data/earning_call/4509360   \n",
       "5   4508080  data/earning_call/4508080   \n",
       "6   4509536  data/earning_call/4509536   \n",
       "7   4508410  data/earning_call/4508410   \n",
       "8   4508622  data/earning_call/4508622   \n",
       "9   4509394  data/earning_call/4509394   \n",
       "10  4508848  data/earning_call/4508848   \n",
       "11  4508074  data/earning_call/4508074   \n",
       "12  4508428  data/earning_call/4508428   \n",
       "13  4508870  data/earning_call/4508870   \n",
       "14  4509358  data/earning_call/4509358   \n",
       "15  4508884  data/earning_call/4508884   \n",
       "16  4509393  data/earning_call/4509393   \n",
       "17  4508625  data/earning_call/4508625   \n",
       "18  4509367  data/earning_call/4509367   \n",
       "19  4508613  data/earning_call/4508613   \n",
       "\n",
       "                                          top_n_words  \n",
       "0   [savings, hardware, app, 3pl, cogs, us, issues...  \n",
       "1   [oneweb, chris, us, government, henry, caleb, ...  \n",
       "2   [patients, us, oxybates, nda, avadel, ag, sodi...  \n",
       "3   [customers, erik, aws, eps, capabilities, chad...  \n",
       "4   [brooks, notes, clinicians, customers, us, all...  \n",
       "5   [ounces, aris, amounted, trevor, haytham, rose...  \n",
       "6   [merchants, originations, cupito, consumers, v...  \n",
       "7   [us, patients, valves, centers, physicians, tr...  \n",
       "8   [sales, increases, heppenstall, constant, impl...  \n",
       "9   [events, customers, planners, hotels, operator...  \n",
       "10  [ounces, recoveries, del, fruta, ore, explorat...  \n",
       "11  [burrows, customers, alberta, northeast, kwan,...  \n",
       "12  [flows, advisors, seed, clients, pension, mcin...  \n",
       "13  [customers, operators, sutton, yoke, ravi, mik...  \n",
       "14  [patients, mutations, pathways, populations, c...  \n",
       "15  [sales, doctors, patients, practices, procedur...  \n",
       "16  [proteins, modifications, kyle, molecule, tech...  \n",
       "17  [surgeons, patients, procedures, sales, adduct...  \n",
       "18  [patients, tfi, year, responder, otonomy, call...  \n",
       "19  [clinics, openings, rd, owned, comps, franchis...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbd5e6",
   "metadata": {},
   "source": [
    "### FinBERT Julien Cyrus\n",
    "https://pypi.org/project/finbert-embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420238b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import BertModel\n",
    "from transformers import BertForSequenceClassification\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbe0f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "num_labels= len(labels)\n",
    "vocab = \"finance-uncased\"\n",
    "vocab_path = 'analyst_tone/vocab'\n",
    "pretrained_weights_path = \"analyst_tone/pretrained_weights\" # this is pre-trained FinBERT weights\n",
    "fine_tuned_weight_path = \"analyst_tone/fine_tuned.pth\"      # this is fine-tuned FinBERT weights\n",
    "max_seq_length=512\n",
    "device='cuda:1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e767cf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/earning_call/4509509', 'data/earning_call/4508877', 'data/earning_call/4509199', 'data/earning_call/4507655', 'data/earning_call/4509360', 'data/earning_call/4508080', 'data/earning_call/4509536', 'data/earning_call/4508410', 'data/earning_call/4508622', 'data/earning_call/4509394', 'data/earning_call/4508848', 'data/earning_call/4508074', 'data/earning_call/4508428', 'data/earning_call/4508870', 'data/earning_call/4509358', 'data/earning_call/4508884', 'data/earning_call/4509393', 'data/earning_call/4508625', 'data/earning_call/4509367', 'data/earning_call/4508613']\n"
     ]
    }
   ],
   "source": [
    "headlines_list = list_articles\n",
    "print(headlines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e8d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650b269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a shortage of capital, and we need extra financing ---- negative\n",
      "growth is strong and we have plenty of liquidity ---- positive\n",
      "there are doubts about our finances ---- negative\n",
      "profits are flat ---- neutral\n",
      "everything is fine ---- positive\n",
      "everything is worse ---- negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\", \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\",\"everything is fine\",\"everything is worse\"]\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "outputs = finbert(**inputs)[0]\n",
    "\n",
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "for idx, sent in enumerate(sentences):\n",
    "    print(sent, '----', labels[np.argmax(outputs.detach().numpy()[idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a41e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "a = 0\n",
    "\n",
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "\n",
    "# Create a dataframe with nb_positive sentence, neutral and negative sentences\n",
    "df_sentiment = pd.DataFrame(columns = [\"Neutral\",\"Positive\", \"Negative\",\"Difference\",\"P/N ratio\", \"Difference ratio\",\n",
    "                                      \"Total\"])\n",
    "\n",
    "\n",
    "for s in list_articles:\n",
    "    with open(s) as f:\n",
    "        t = f.read()\n",
    "        x = split_into_sentences(t)\n",
    "        a = a+1\n",
    "        print(a)\n",
    "        inputs = tokenizer(x, return_tensors=\"pt\", padding=True)\n",
    "        outputs = finbert(**inputs)[0] # Prends bcp de temps. pour run\n",
    "        \n",
    "           \n",
    "        # [0] neutral\n",
    "        #[1] positive\n",
    "        # [2] negative\n",
    "        \n",
    "        nb_neutral = nb_positive = nb_negative = 0\n",
    "        \n",
    "    \n",
    "        for idx,elem in enumerate(x):\n",
    "            #print(outputs.detach().numpy()[idx])\n",
    "            #print(elem, '----', labels[np.argmax(outputs.detach().numpy()[idx])])\n",
    "            #print(labels[np.argmax(outputs.detach().numpy()[idx])])\n",
    "            \n",
    "            sentiment = labels[np.argmax(outputs.detach().numpy()[idx])]\n",
    "            \n",
    "            if(sentiment == \"neutral\"):\n",
    "                nb_neutral += 1\n",
    "            elif(sentiment == \"positive\"):\n",
    "                nb_positive += 1\n",
    "            elif(sentiment == \"negative\"):\n",
    "                nb_negative += 1\n",
    "            else :\n",
    "                pass\n",
    "        \n",
    "        total = nb_neutral + nb_positive + nb_negative\n",
    "        difference = nb_positive - nb_negative\n",
    "        P_N = nb_positive/nb_negative\n",
    "        difference_ratio = difference / total\n",
    "        \n",
    "        df_sentiment.loc[len(df_sentiment.index)] = [nb_neutral,nb_positive,nb_negative, difference, \n",
    "                                                    P_N,difference_ratio, total]\n",
    "        #display(df_sentiment) \n",
    "\n",
    "df_sentiment = df_sentiment.astype({\"Neutral\": int, \"Positive\": int, \"Negative\" : int, \"Difference\" : int,\n",
    "                                   \"Total\": int})\n",
    "\n",
    "display(df_sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79699c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment[\"Final Sentiment\"] = None\n",
    "\n",
    "\n",
    "for index, row in df_sentiment.iterrows():\n",
    "    #print(row['c1'], row['c2'])\n",
    "    \n",
    "    if row[\"Difference ratio\"] > 0.2 :\n",
    "        df_sentiment[\"Final Sentiment\"] = \"Positive\"\n",
    "    \n",
    "    elif row[\"Difference ratio\"] < 0.1 :\n",
    "        df_sentiment[\"Final Sentiment\"] = \"Negative\"\n",
    "        \n",
    "    else :\n",
    "         df_sentiment[\"Final Sentiment\"] = \"Neutral\"\n",
    "\n",
    "            \n",
    "display(df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment[\"Decile\"] = 1 + df_sentiment[\"Difference ratio\"].transform(lambda y: pd.qcut(y, 10, labels=False))\n",
    "df_sentiment= np.round(df_sentiment,5)\n",
    "display(df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e92087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment.to_csv('data_sentiment.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4f840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}