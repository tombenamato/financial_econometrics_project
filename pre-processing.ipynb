{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf2c61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections, itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1173f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/text_article_4507624.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc99b2",
   "metadata": {},
   "source": [
    "0. Function that transforms a list of strings into 1 big concatenated string and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd085b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_text(list_input, stops = []):\n",
    "    text_output = ' '.join([word for word in list_input if word not in stops]) \n",
    "    return text_output\n",
    "\n",
    "def text_to_list(text_input):\n",
    "    list_output = word_tokenize(text_input)\n",
    "    return list_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d75d0",
   "metadata": {},
   "source": [
    "1. Ponctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86874a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('[^A-Za-z0-9]+', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cba145",
   "metadata": {},
   "source": [
    "2. Lower all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c46fbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76912038",
   "metadata": {},
   "source": [
    "3. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c8692d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before removing the stop words 6192\n",
      "Number of words after removing the stop words 3165\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text)\n",
    "filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "print('Number of words before removing the stop words',len(word_tokens))\n",
    "print('Number of words after removing the stop words',len(filtered_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24908ca0",
   "metadata": {},
   "source": [
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73b9710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "331b27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize only the noun\n",
    "x = [wordnet_lemmatizer.lemmatize(word, pos='n') for word in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6b9db",
   "metadata": {},
   "source": [
    "5. N-Gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29918d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_list(n):\n",
    "    \"\"\"\n",
    "        Compute ngrams.\n",
    "        \n",
    "        Args:\n",
    "            n (int): the number of words to words to assemble in the ngram.\n",
    "        \n",
    "        Returns :\n",
    "            A list composed of the ngrams.\n",
    "    \"\"\"\n",
    "    m = []\n",
    "    nx_grams = ngrams(sequence = nltk.word_tokenize(text), n = n)\n",
    "    for gram in nx_grams:\n",
    "        m.append(gram)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d29db",
   "metadata": {},
   "source": [
    "6. Remove most frequent and least frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5589f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(a,l,h):\n",
    "    \"\"\"\n",
    "        Remove most and least frequent words.\n",
    "        Args:\n",
    "            a (list) : list on which operations should be made.\n",
    "            l (float): the proportion of top l% least frequent words to remove from the numer of different words.\n",
    "            h (float): the proportion of top h% most frequent words to remove from the numer of different words.\n",
    "    \n",
    "        Returns:\n",
    "            A copy of the input text without frequent and infrequent words.\n",
    "    \"\"\"\n",
    "    f = FreqDist(a)\n",
    "\n",
    "    df_fdist = pd.DataFrame({'Word': f.keys(), 'Number of apparitions': f.values()})\n",
    "    L= l*len(df_fdist)\n",
    "    L=int(L)\n",
    "\n",
    "    H=h*len(df_fdist)\n",
    "    H=int(H)\n",
    "    \n",
    "    df_fdesc = df_fdist.sort_values(by='Number of apparitions', ascending=False)\n",
    "    df_fasc = df_fdist.sort_values(by='Number of apparitions', ascending=True)\n",
    "\n",
    "    most_freq_words_list = list(df_fdesc['Word'][:H])\n",
    "    least_freq_word_list = list(df_fasc['Word'][:L])\n",
    "    stopwords = most_freq_words_list + least_freq_word_list\n",
    "    textlist_wo_extremes = list_to_text(filtered_text, stopwords)\n",
    "    #text_wo_extremes = ' '.join([word for word in filtered_text if word not in stopwords]) \n",
    "\n",
    "    return textlist_wo_extremes, df_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "536579e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = remove(filtered_text,0.03,0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "485f7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1048082",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695ca7b",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "06ff1dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1818)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [result_1[0],result_2[0]]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba16db",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)\n",
    "Using the TFIDF matrix, perform an SVD to keep only the k topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cfa914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3c27eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics or components\n",
    "num_components=50\n",
    "\n",
    "# Create SVD object\n",
    "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
    "\n",
    "# Fit SVD model on data\n",
    "lsa.fit_transform(X)\n",
    "\n",
    "# Get Singular values and Components \n",
    "Sigma = lsa.singular_values_ \n",
    "V_transpose = lsa.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2a6aad18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.90828416e-29, 1.00000000e+00])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1ce0cb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1818,)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(lsa.components_))[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "84ad716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics with their terms\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "64dd9126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:  ['last', 'forward', 'know', 'consumption', 'continue']\n",
      "Document 1:  ['consumption', 'continue', 'inflation', 'really', 'brand']\n"
     ]
    }
   ],
   "source": [
    "for doc, component in enumerate(lsa.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Document \"+str(doc)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb2189",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "706245ae",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "--> supervised learning !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05797e",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "#to do how data cleanned / words removeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9366762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/tatianacogne/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /Users/tatianacogne/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4f4cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "import gensim\n",
    "import string\n",
    "from gensim import corpora\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b466451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc = [document.split() for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a98d14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(final_doc)\n",
    "DT_matrix = [dictionary.doc2bow(doc) for doc in final_doc]\n",
    "Lda_object = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "211267c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DT_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbc3d4",
   "metadata": {},
   "source": [
    "- num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "457d07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.003*\"last\" + 0.002*\"forward\" + 0.002*\"side\" + 0.002*\"term\" + 0.002*\"around\" + 0.002*\"sort\" + 0.002*\"marketing\" + 0.002*\"maybe\" + 0.002*\"within\" + 0.002*\"existing\"'), (1, '0.003*\"know\" + 0.003*\"demand\" + 0.003*\"way\" + 0.003*\"forward\" + 0.002*\"looking\" + 0.002*\"marketing\" + 0.002*\"prior\" + 0.002*\"little\" + 0.002*\"continue\" + 0.002*\"last\"')]\n"
     ]
    }
   ],
   "source": [
    "lda_model_1 = Lda_object(DT_matrix, num_topics=2, id2word = dictionary)\n",
    "print(lda_model_1.show_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ef3c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6d13d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8ed4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
