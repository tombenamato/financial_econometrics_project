{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2c61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ellietupin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ellietupin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ellietupin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ellietupin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections, itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1173f48e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/text_article_4484494.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/zvz37scs6vj6_wyjn348qj800000gn/T/ipykernel_3382/764759588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/text_article_4484494.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/text_article_4484494.txt'"
     ]
    }
   ],
   "source": [
    "with open('data/text_article_4484494.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d75d0",
   "metadata": {},
   "source": [
    "1. Ponctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86874a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('[^A-Za-z0-9]+', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cba145",
   "metadata": {},
   "source": [
    "2. Lower all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46fbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76912038",
   "metadata": {},
   "source": [
    "3. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8692d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text)\n",
    "filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "print('Number of words before removing the stop words',len(word_tokens))\n",
    "print('Number of words after removing the stop words',len(filtered_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24908ca0",
   "metadata": {},
   "source": [
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize only the noun\n",
    "x = [wordnet_lemmatizer.lemmatize(word, pos='n') for word in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6b9db",
   "metadata": {},
   "source": [
    "5. N-Gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29918d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_list(n):\n",
    "    \"\"\"\n",
    "        Compute ngrams.\n",
    "        \n",
    "        Args:\n",
    "            n (int): the number of words to words to assemble in the ngram.\n",
    "        \n",
    "        Returns :\n",
    "            A list composed of the ngrams.\n",
    "    \"\"\"\n",
    "    m = []\n",
    "    nx_grams = ngrams(sequence = nltk.word_tokenize(text), n = n)\n",
    "    for gram in nx_grams:\n",
    "        m.append(gram)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d29db",
   "metadata": {},
   "source": [
    "6. Remove most frequent and least frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5589f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(a,l,h):\n",
    "    \"\"\"\n",
    "        Remove most and least frequent words.\n",
    "        Args:\n",
    "            a (list) : list on which operations should be made.\n",
    "            l (float): the proportion of top l% least frequent words to remove from the numer of different words.\n",
    "            h (float): the proportion of top h% most frequent words to remove from the numer of different words.\n",
    "    \n",
    "        Returns:\n",
    "            A copy of the input text without frequent and infrequent words.\n",
    "    \"\"\"\n",
    "    f = FreqDist(a)\n",
    "\n",
    "    df_fdist = pd.DataFrame({'Word': f.keys(), 'Number of apparitions': f.values()})\n",
    "    L= l*len(df_fdist)\n",
    "    L=int(L)\n",
    "\n",
    "    H=h*len(df_fdist)\n",
    "    H=int(H)\n",
    "    \n",
    "    df_fdesc = df_fdist.sort_values(by='Number of apparitions', ascending=False)\n",
    "    df_fasc = df_fdist.sort_values(by='Number of apparitions', ascending=True)\n",
    "\n",
    "    most_freq_words_list = list(df_fdesc['Word'][:H])\n",
    "    least_freq_word_list = list(df_fasc['Word'][:L])\n",
    "    stopwords = most_freq_words_list + least_freq_word_list\n",
    "    text_wo_extremes = ' '.join([word for word in filtered_text if word not in stopwords]) \n",
    "\n",
    "    return text_wo_extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536579e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove(filtered_text,0.03,0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f7bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1048082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff1dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057ce85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
