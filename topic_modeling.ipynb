{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a6c2ef",
   "metadata": {},
   "source": [
    "# Topic Modeling \n",
    "This notebook aims to take as input the texts who have been processed and use it to find the most relevants topics and the words that are relevant for the sentimental analysis.\n",
    "\n",
    "**Implementation**\n",
    "- TF-IDF\n",
    "- FinBERT\n",
    "- LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea88cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "from pre_processing import processing\n",
    "from finbert_embedding.embedding import FinbertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd213fc",
   "metadata": {},
   "source": [
    "### Import the text and process it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_articles = glob.glob(\"data/earning_call/*\")\n",
    "texts = []\n",
    "first_sentence = []\n",
    "articles = []\n",
    "for s in list_articles:\n",
    "    with open(s) as f:\n",
    "        x = int(re.sub('data/earning_call/','',s))\n",
    "        articles.append(x)\n",
    "        t = f.read()\n",
    "        texts.append(t)\n",
    "        \n",
    "print('Number of articles', len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf44387",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [processing(x) for x in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b7f4",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_w_index = vectorizer.vocabulary_\n",
    "dict_index_w = {v: k for k, v in dict_w_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87daa161",
   "metadata": {},
   "source": [
    "Take the top n words depending on the score with the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =10\n",
    "top_n = []\n",
    "for i in range(X_tfidf.shape[0]):\n",
    "    index = X_tfidf[i,].nonzero()[1]\n",
    "    words_of_index = [dict_index_w[x] for x in index]\n",
    "    score_of_index = [X_tfidf[i,x] for x in index]\n",
    "    x = list(zip(words_of_index,score_of_index))\n",
    "    x.sort(key=lambda x: -x[1])\n",
    "    a = [w[0] for w in x[:n]]\n",
    "    top_n.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'article':articles,'file_path':list_articles,'top_n_words':top_n})\n",
    "df.to_pickle(\"data/top_n_words_tfidf.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3450ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbd5e6",
   "metadata": {},
   "source": [
    "### FinBERT \n",
    "https://pypi.org/project/finbert-embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10848b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = FinbertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fffa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FinB = np.zeros((len(texts),768))\n",
    "k=0\n",
    "for text in texts:\n",
    "    X_FinB[k,] = finbert.sentence_vector(text)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FinB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1fdda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
