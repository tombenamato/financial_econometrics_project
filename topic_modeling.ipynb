{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a6c2ef",
   "metadata": {},
   "source": [
    "# Topic Modeling \n",
    "This notebook aims to take as input the texts who have been processed and use it to find the most relevants topics and the words that are relevant for the sentimental analysis.\n",
    "\n",
    "**Implementation**\n",
    "- TF-IDF\n",
    "- FinBERT\n",
    "- LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea88cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from pre_processing.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Roger\n",
      "[nltk_data]     Gualberti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Roger\n",
      "[nltk_data]     Gualberti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Roger\n",
      "[nltk_data]     Gualberti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Roger\n",
      "[nltk_data]     Gualberti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'data/earning_call\\\\4427353'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimport_ipynb\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpre_processing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m processing\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinbert_embedding\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FinbertEmbedding\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:664\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:627\u001b[0m, in \u001b[0;36m_load_backward_compatible\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\import_ipynb.py:61\u001b[0m, in \u001b[0;36mNotebookLoader.load_module\u001b[1;34m(self, fullname)\u001b[0m\n\u001b[0;32m     59\u001b[0m         code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39minput_transformer_manager\u001b[38;5;241m.\u001b[39mtransform_cell(cell\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# run the code in themodule\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m         \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns \u001b[38;5;241m=\u001b[39m save_user_ns\n",
      "File \u001b[1;32m<string>:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'data/earning_call\\\\4427353'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "from pre_processing import processing\n",
    "from finbert_embedding.embedding import FinbertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd213fc",
   "metadata": {},
   "source": [
    "### Import the text and process it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_articles = glob.glob(\"data/earning_call/*\")\n",
    "texts = []\n",
    "first_sentence = []\n",
    "articles = []\n",
    "for s in list_articles:\n",
    "    with open(s) as f:\n",
    "        x = int(re.sub('data/earning_call/','',s))\n",
    "        articles.append(x)\n",
    "        t = f.read()\n",
    "        texts.append(t)\n",
    "        \n",
    "print('Number of articles', len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf44387",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [processing(x) for x in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b7f4",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_w_index = vectorizer.vocabulary_\n",
    "dict_index_w = {v: k for k, v in dict_w_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87daa161",
   "metadata": {},
   "source": [
    "Take the top n words depending on the score with the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =10\n",
    "top_n = []\n",
    "for i in range(X_tfidf.shape[0]):\n",
    "    index = X_tfidf[i,].nonzero()[1]\n",
    "    words_of_index = [dict_index_w[x] for x in index]\n",
    "    score_of_index = [X_tfidf[i,x] for x in index]\n",
    "    x = list(zip(words_of_index,score_of_index))\n",
    "    x.sort(key=lambda x: -x[1])\n",
    "    a = [w[0] for w in x[:n]]\n",
    "    top_n.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'article':articles,'file_path':list_articles,'top_n_words':top_n})\n",
    "df.to_pickle(\"data/top_n_words_tfidf.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3450ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbd5e6",
   "metadata": {},
   "source": [
    "### FinBERT \n",
    "https://pypi.org/project/finbert-embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10848b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = FinbertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fffa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FinB = np.zeros((len(texts),768))\n",
    "k=0\n",
    "for text in texts:\n",
    "    X_FinB[k,] = finbert.sentence_vector(text)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FinB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1fdda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
