{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1640f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries as needed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm.notebook import tqdm # to create loadbard in for loop\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29a3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_for_scrap(ticker, publish_time_min ):\n",
    "    # accept publis_time_min = None\n",
    "    return_per_request  = 20 #the one used by seekingalpha by default\n",
    "    ticker = ticker.lower()\n",
    "    publish_time_min = \"undefined\" if publish_time_min ==None else str(publish_time_min)\n",
    "    to_return = (\"https://seekingalpha.com/api/v3/symbols/\"+ticker+\"/transcripts?filter[until]=\"+publish_time_min\n",
    "                 +\"&id=\"+ticker\n",
    "                 +\"&include=author%2CprimaryTickers%2CsecondaryTickers%2Csentiments&isMounting=true&page[size]=\"\n",
    "                 + str(return_per_request))\n",
    "    return to_return\n",
    "def is_earning_call(element_response):\n",
    "    title = element_response[\"attributes\"][\"title\"].lower()\n",
    "    has_earning_in_title = \"earning\" in title or \"earnings\" in title\n",
    "    has_call_in_title = \"call\" in title\n",
    "    return element_response[\"type\"]==\"transcript\" and has_call_in_title and has_earning_in_title\n",
    "def get_url_id_date_earning_call(element_response):\n",
    "    date = element_response[\"attributes\"][\"publishOn\"][:10]\n",
    "    call_id = element_response[\"id\"]\n",
    "    url = element_response[\"links\"][\"self\"]\n",
    "    return url, call_id, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7d0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_tickers():\n",
    "    table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    df_tickers = table[0]\n",
    "    tickers=df_tickers[\"Symbol\"].values\n",
    "    tickers= tickers.tolist()\n",
    "    return tickers\n",
    "\n",
    "def get_sp600_tickers():\n",
    "    table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_600_companies#S&P_600_Component_Stocks')\n",
    "    df_tickers = table[1]\n",
    "    tickers = df_tickers['Ticker symbol'].values\n",
    "    tickers = tickers.tolist()\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547c84b",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10af4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAN', 'AAON']\n"
     ]
    }
   ],
   "source": [
    "# CELL TO CHANGE\n",
    "executable_path = './chromedriver_linux'\n",
    "ticker_html_path = \"data/sp600/ticker/\"\n",
    "tickers = get_sp600_tickers() \n",
    "tickers = tickers[:2]\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffaf711",
   "metadata": {},
   "source": [
    "## Scrap HTMLs\n",
    "Scrap HTML if not already scrapped and also save a dictionary pickle of key earning id and value date of transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e04f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaf74418e3d4578a0d6e1a72978fd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for now scrapp all earning call transcript from one company\n",
    "for ticker in tqdm(tickers):\n",
    "    html_path = ticker_html_path + ticker \n",
    "    last_time_scrap_path = html_path+ \"/last_time_scrap\"\n",
    "    if not os.path.exists(html_path):\n",
    "        os.makedirs(html_path)\n",
    "        print(f\"The new directory {html_path} has been created!\")\n",
    "        \n",
    "    if os.path.exists(last_time_scrap_path):\n",
    "        with open(last_time_scrap_path, \"r\") as file:\n",
    "            publish_time_min = file.read()\n",
    "    else : \n",
    "        publish_time_min = None\n",
    "    if publish_time_min !=\"all\":\n",
    "        more_data = True\n",
    "        id_URLs = []\n",
    "        id_to_date = {}\n",
    "        while more_data:\n",
    "            service = Service(executable_path)\n",
    "            option = webdriver.ChromeOptions()\n",
    "            driver = webdriver.Chrome(service=service,options=option)\n",
    "            url = create_url_for_scrap(ticker, publish_time_min)\n",
    "            driver.get(url)\n",
    "            json_response = driver.find_element(by=By.TAG_NAME, value = \"body\").text\n",
    "            data_responses = json.loads(json_response)\n",
    "            if \"meta\" not in data_responses:\n",
    "                time.sleep(600) # 2 chances to be flag as robot\n",
    "                driver.get(url)\n",
    "                json_response = driver.find_element(by=By.TAG_NAME, value = \"body\").text\n",
    "                data_responses = json.loads(json_response)\n",
    "                if \"meta\" not in data_responses:\n",
    "                    last_publish_time_min = publish_time_min\n",
    "                    break\n",
    "            publish_time_min = str(data_responses[\"meta\"][\"page\"][\"minmaxPublishOn\"][\"min\"]) # need to be string\n",
    "            if publish_time_min!= 'None':\n",
    "                for element in data_responses[\"data\"]:\n",
    "                    if is_earning_call(element):\n",
    "                        URL, call_id, date = get_url_id_date_earning_call(element)\n",
    "                        id_URLs.append((call_id,URL))\n",
    "                        id_to_date[call_id] = date\n",
    "            else :\n",
    "                last_publish_time_min = \"all\"\n",
    "                more_data = False\n",
    "            time.sleep(2+random.uniform(0, 1))\n",
    "            driver.close()\n",
    "            \n",
    "        # Download HTML files\n",
    "        domain_name = 'https://seekingalpha.com'\n",
    "        for call_id, URL in tqdm(id_URLs):\n",
    "            file_path = html_path + \"/\"+ call_id + \".html\"\n",
    "            with open(file_path, \"w\") as file :\n",
    "                url = domain_name+URL\n",
    "                html =  requests.get(url).text\n",
    "                file.write(html)\n",
    "\n",
    "        # Save the last scrapped url of the ticker, so that if bot detected can resume\n",
    "        with open(last_time_scrap_path, \"w\") as file:\n",
    "            file.write(last_publish_time_min)    \n",
    "\n",
    "\n",
    "        # Save date of transcripts\n",
    "        id_to_date_path = ticker_html_path + \"id_to_date\"\n",
    "        if not os.path.exists(id_to_date_path):\n",
    "            file_id_to_date = {}\n",
    "        else:\n",
    "            with open(id_to_date_path,\"rb\") as file :\n",
    "                file_id_to_date = pickle.load(file)\n",
    "        for call_id, date in id_to_date.items():\n",
    "            file_id_to_date[call_id] = date\n",
    "\n",
    "        with open(id_to_date_path,\"wb\") as file : \n",
    "            pickle.dump(file_id_to_date, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b0260",
   "metadata": {},
   "source": [
    "## Check last file got downloaded, i.e. no captcha/bot error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4d6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(html).get_text()[:2000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
