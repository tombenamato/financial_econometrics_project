{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c61dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "import collections, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import hdbscan\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09874bea",
   "metadata": {},
   "source": [
    "### Import the text and process it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_articles = glob.glob(\"data/earning_call/*\")\n",
    "texts = []\n",
    "first_sentence = []\n",
    "articles = []\n",
    "for s in list_articles:\n",
    "    with open(s) as f:\n",
    "        x = int(re.sub('data/earning_call/','',s))\n",
    "        articles.append(x)\n",
    "        t = f.read()\n",
    "        texts.append(t)\n",
    "print('Number of articles', len(texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f454ad1",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that transforms a list of strings into 1 big concatenated string and vice-versa\n",
    "def list_to_text(list_input, stops = []):\n",
    "    text_output = ' '.join([word for word in list_input if word not in stops]) \n",
    "    return text_output\n",
    "\n",
    "def text_to_list(text_input):\n",
    "    list_output = word_tokenize(text_input)\n",
    "    return list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c343dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "# Ponctuation\n",
    "text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "\n",
    "# Lower all words\n",
    "text = text.lower()\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text)\n",
    "filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "print('Number of words before removing the stop words',len(word_tokens))\n",
    "print('Number of words after removing the stop words',len(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram\n",
    "def ngrams_list(n):\n",
    "    \"\"\"\n",
    "        Compute ngrams.\n",
    "        \n",
    "        Args:\n",
    "            n (int): the number of words to words to assemble in the ngram.\n",
    "        \n",
    "        Returns :\n",
    "            A list composed of the ngrams.\n",
    "    \"\"\"\n",
    "    m = []\n",
    "    nx_grams = ngrams(sequence = nltk.word_tokenize(text), n = n)\n",
    "    for gram in nx_grams:\n",
    "        m.append(gram)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove most frequent and least frequent words\n",
    "def remove(filtered_text, a, l, h):\n",
    "    \"\"\"\n",
    "        Remove most and least frequent words.\n",
    "        Args:\n",
    "            a (list) : list on which operations should be made.\n",
    "            l (float): the proportion of top l% least frequent words to remove from the numer of different words.\n",
    "            h (float): the proportion of top h% most frequent words to remove from the numer of different words.\n",
    "    \n",
    "        Returns:\n",
    "            A copy of the input text without frequent and infrequent words.\n",
    "    \"\"\"\n",
    "    f = FreqDist(a)\n",
    "\n",
    "    df_fdist = pd.DataFrame({'Word': f.keys(), 'Number of apparitions': f.values()})\n",
    "    L= l*len(df_fdist)\n",
    "    L=int(L)\n",
    "\n",
    "    H=h*len(df_fdist)\n",
    "    H=int(H)\n",
    "    \n",
    "    df_fdesc = df_fdist.sort_values(by='Number of apparitions', ascending=False)\n",
    "    df_fasc = df_fdist.sort_values(by='Number of apparitions', ascending=True)\n",
    "\n",
    "    most_freq_words_list = list(df_fdesc['Word'][:H])\n",
    "    least_freq_word_list = list(df_fasc['Word'][:L])\n",
    "    stopwords = most_freq_words_list + least_freq_word_list\n",
    "    textlist_wo_extremes = list_to_text(filtered_text, stopwords)\n",
    "    #text_wo_extremes = ' '.join([word for word in filtered_text if word not in stopwords]) \n",
    "\n",
    "    return textlist_wo_extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536579e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processing(text):\n",
    "    \"\"\"Function that combien all the processing steps\"\"\"\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    x = [wordnet_lemmatizer.lemmatize(word, pos='n') for word in filtered_text]\n",
    "    return remove(filtered_text, x,0.06,0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of articles to train \n",
    "texts = [processing(x) for x in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cb091",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63549a72",
   "metadata": {},
   "source": [
    "# Define the number of topics or components\n",
    "num_components=5\n",
    "\n",
    "# Create SVD object\n",
    "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
    "\n",
    "# Fit SVD model on data\n",
    "lsa.fit_transform(X_tfidf)\n",
    "\n",
    "# Get Singular values and Components \n",
    "Sigma = lsa.singular_values_ \n",
    "V_transpose = lsa.components_.T\n",
    "\n",
    "# Print the topics with their terms\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for doc, component in enumerate(lsa.components_):\n",
    "    print(doc)\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:15]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Document \"+str(doc)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f5b1e",
   "metadata": {},
   "source": [
    "## Randomized LSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837bde09",
   "metadata": {},
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "U, Sigma, VT = randomized_svd(X_tfidf, \n",
    "                              n_components=15,\n",
    "                              n_iter=5,\n",
    "                              random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2086ecf",
   "metadata": {},
   "source": [
    "U[0:2,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc50ded",
   "metadata": {},
   "source": [
    "### Dimensionality reduction and clustering before topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93733c1c",
   "metadata": {},
   "source": [
    "embeddings = X_FinB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cbe31",
   "metadata": {},
   "source": [
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95830028",
   "metadata": {},
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1835e34",
   "metadata": {},
   "source": [
    "umap_embeddings = umap.UMAP(n_components= 15, n_neighbors=15, metric='cosine').fit_transform(embeddings)\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=5,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f19ab1",
   "metadata": {},
   "source": [
    "'Number of clusters/topics ', len(set(cluster.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c213e8",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.5)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=3, cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ce73b",
   "metadata": {},
   "source": [
    "docs_df = pd.DataFrame(texts, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bff9cd",
   "metadata": {},
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(texts[:n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446262f0",
   "metadata": {},
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439ab40",
   "metadata": {},
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c71af8",
   "metadata": {},
   "source": [
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57197bc",
   "metadata": {},
   "source": [
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376d60a",
   "metadata": {},
   "source": [
    "k=0\n",
    "for t in topic_sizes.Topic.values:\n",
    "    if(t!=-1):\n",
    "        top_n_words[t].sort(key=lambda x:- x[1])\n",
    "        k +=1\n",
    "        print('')\n",
    "        print('Topic - ',k)\n",
    "        for i in range(10):\n",
    "            print(top_n_words[t][i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706245ae",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "--> supervised learning !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05797e",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "#to do how data cleanned / words removeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8f401",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84c2b6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "final_doc = [document.split() for document in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755ab15",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "dictionary = corpora.Dictionary(final_doc)\n",
    "DT_matrix = [dictionary.doc2bow(doc) for doc in final_doc]\n",
    "Lda_object = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4f306",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "lda_model_1 = Lda_object(DT_matrix, num_topics=2, id2word = dictionary)\n",
    "print(lda_model_1.show_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130e33d",
   "metadata": {},
   "source": [
    "## BERT - Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632e915",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "embeddings = model.encode(texts[:10], show_progress_bar=True)\n",
    "\n",
    "embeddings,shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
