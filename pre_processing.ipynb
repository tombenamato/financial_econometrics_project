{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec66965a",
   "metadata": {},
   "source": [
    "# Processing\n",
    "This notebooks contains all the functions needed to download all the texts and also process them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2c61dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tatianacogne/opt/anaconda3/envs/res/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09874bea",
   "metadata": {},
   "source": [
    "## Import the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936cdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_texts():\n",
    "    \"\"\"Function that will find all the earning calls downloaded and import them. It returns a list with all the texts.\"\"\"\n",
    "    dir_path = \"data/text/\"\n",
    "    list_tickers = os.listdir(dir_path)\n",
    "    texts = []\n",
    "    articles = []\n",
    "    compagny = []\n",
    "    list_tickers.remove('.DS_Store')\n",
    "    for ticker in list_tickers:\n",
    "        earning_call_path = \"data/text/\"+ticker\n",
    "        list_articles = os.listdir(earning_call_path)\n",
    "        \n",
    "        list_articles = [earning_call_path+'/'+x for x in list_articles if x!='.DS_Store' and x!= '.ipynb_checkpoints']\n",
    "        for s in list_articles:\n",
    "            with open(s) as f:\n",
    "                x = int(re.sub(earning_call_path+'/','',s))\n",
    "                articles.append(x)\n",
    "                t = f.read()\n",
    "                texts.append(t)\n",
    "                compagny.append(ticker)\n",
    "        \n",
    "    print('Number of articles', len(texts))\n",
    "    \n",
    "    df = pd.DataFrame({'ticker':compagny,'article':articles,'text':texts})\n",
    "    df.text = df.text.apply(lambda x : re.sub('Question-and-Answer Session','',x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f454ad1",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d24d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_text(list_input, stops = []):\n",
    "    \"Function that take as input a text that have been tokenized and put it back into one single string.\"\n",
    "    text_output = ' '.join([word for word in list_input if word not in stops]) \n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248f652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list(text_input):\n",
    "    \"\"\"Function that takes as input a text and tokenize it.\"\"\"\n",
    "    list_output = word_tokenize(text_input)\n",
    "    return list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f363f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_list(n):\n",
    "    \"\"\"Function that takes as input n which correspond to the blocks that we want to do with n-grams.\"\"\"\n",
    "    m = []\n",
    "    nx_grams = ngrams(sequence = nltk.word_tokenize(text), n = n)\n",
    "    for gram in nx_grams:\n",
    "        m.append(gram)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006e0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_speakers(text):\n",
    "    \"\"\"Function ta remove the speakers from one text for the first option on FiGAS.\"\"\"\n",
    "    sentences = []\n",
    "    list_string = text.split('\\n \\n')\n",
    "    for s in list_string:\n",
    "        if(len(s.split(' '))>5):\n",
    "            sentences.append(s)\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847c701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_speakers2(text):\n",
    "    \"\"\"Function ta remove the speakers from one text for the second option.\"\"\"\n",
    "    sentences = []\n",
    "    list_string = text.split('\\n \\n')\n",
    "    for s in list_string:\n",
    "        if(len(s.split(' '))>5):\n",
    "            sentences.append(s)\n",
    "    return '||'.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad992b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57bcc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_process(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4b4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_most_least_freq(list_words, lower, upper):\n",
    "    \"\"\"Remove most and least frequent words with a given lower (l) and upper (u) bound to remove certain percentage of occurences.\"\"\"\n",
    "    f = FreqDist(list_words)\n",
    "    df_fdist = pd.DataFrame({'Word': f.keys(), 'Number of apparitions': f.values()})\n",
    "    num_lower=int(lower*len(df_fdist))\n",
    "    num_upper=int(upper*len(df_fdist))\n",
    "    \n",
    "    vocabulary = Counter(list_words)\n",
    "    sorted_vocabulary = sorted(vocabulary.items(), key=lambda x: x[1])\n",
    "    most_common = sorted_vocabulary[-num_upper:][::-1]\n",
    "    least_common = sorted_vocabulary[:num_lower]\n",
    "    stopwords = [x[0] for x in most_common+least_common]\n",
    "    \n",
    "    return list_to_text(list_words, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe11d40",
   "metadata": {},
   "source": [
    "### Processing FiGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c26399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_figas(text):\n",
    "    \"\"\"Function that combine all the processing steps for the first option on FiGAS.\"\"\"\n",
    "    filter_speaker = remove_speakers(text)\n",
    "    filter_char = sentences_process(filter_speaker)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(filter_char)\n",
    "    filter_stopwords = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    tagged = nltk.pos_tag(filter_stopwords)\n",
    "    filter_noun = [w[0] for w in tagged if w[1] !='NNP']\n",
    "    filter_freq = remove_most_least_freq(filter_noun, 0.06, 0.06)\n",
    "    tokenization = word_tokenize(filter_freq)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatization = ' '.join([lemmatizer.lemmatize(w) for w in tokenization])\n",
    "    \n",
    "    return lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd69487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_figas2(text):\n",
    "    \"\"\"Function that combine all the processing steps for the second option on FiGAS.\"\"\"\n",
    "    filter_speaker = remove_speakers2(text)\n",
    "    filter_char = sentences_process(filter_speaker)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(filter_char)\n",
    "    filter_stopwords = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    tagged = nltk.pos_tag(filter_stopwords)\n",
    "    filter_noun = [w[0] for w in tagged if w[1] !='NNP']\n",
    "    filter_freq = remove_most_least_freq(filter_noun, 0.06, 0.06)\n",
    "    tokenization = word_tokenize(filter_freq)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatization = ' '.join([lemmatizer.lemmatize(w) for w in tokenization])\n",
    "    \n",
    "    return lemmatization"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8afb20c03eb43ee960a6eb9b47a880f3e5ef6fa8a91dccdd9bd06197b17763f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('res')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
